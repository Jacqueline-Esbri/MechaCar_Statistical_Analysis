{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module_16_Work.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPFbscHLHC6A",
        "outputId": "4ef445d6-e9b8-4fd0-b45b-de759a74fff2"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.2'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Wait\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,154 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,414 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,770 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,586 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,184 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [906 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Fetched 11.3 MB in 4s (2,634 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEOCXYTITJHn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6BBuinGTQhz"
      },
      "source": [
        "**16.4.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyCMByQkL_S4"
      },
      "source": [
        "\n",
        "# Start Spark session\n",
        "# Creates a Spark application called DataFrameBasics\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rFjW-_N5N2",
        "outputId": "7f4bf9c2-55f4-4cf7-d4a4-92bdb5dc7156"
      },
      "source": [
        "# Example to Create DataFrame\n",
        "dataframe = spark.createDataFrame([ \n",
        "                              (0, \"Here is our DataFrame\"),\n",
        "                              (1, \"We are making one from scratch\"),\n",
        "                              (2, \"This will look very similar to pandas DataFrame\")\n",
        "  ], [ \"id\", \"words\"])\n",
        "dataframe.show()\n",
        "                                   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|               words|\n",
            "+---+--------------------+\n",
            "|  0|Here is our DataF...|\n",
            "|  1|We are making one...|\n",
            "|  2|This will look ve...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHCkr-WKNDqb"
      },
      "source": [
        "This Following code tells Spark to pull data from Amazon's Simple \n",
        "Storage Service (S3), a cloud-based data storage service."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUfPcC6vQrUV"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSzEjNXHRvD5",
        "outputId": "f8d4edba-eb70-40cd-91ad-2ba426ce2fee"
      },
      "source": [
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|   food|price|\n",
            "+-------+-----+\n",
            "|  pizza|    0|\n",
            "|  sushi|   12|\n",
            "|chinese|   10|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNazVf_9R70L",
        "outputId": "16d25ffe-14b1-48a9-90cf-7abcc6056531"
      },
      "source": [
        "# To check the schema \n",
        "# Print our schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- food: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8aCoAEiSLSS",
        "outputId": "d06137aa-f97a-4938-ce62-191b3459d97f"
      },
      "source": [
        "# Show the columns\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['food', 'price']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpQgbPspSVwj",
        "outputId": "aaaa131c-b966-4991-bb81-5b54217eea90"
      },
      "source": [
        "# Describe our data\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, food: string, price: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMfRT2GASkLD"
      },
      "source": [
        "# Price is showing as string ( we need to change this column)\n",
        "# We'll start by importing the different types of data with the following code:\n",
        "\n",
        "# Import struct fields that we can use\n",
        "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm3Qu_KrM5_e"
      },
      "source": [
        "Next, create the schema by creating a StructType, which is one of \n",
        "Spark's complex types, like an array or map. The StructField will \n",
        "define the column name, the data type held, and a Boolean to define \n",
        "whether null values will be included or not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO3M4lUnUk_d",
        "outputId": "ec4f25d4-bad0-4b7c-88f7-efbf46701286"
      },
      "source": [
        "# Next we need to create the list of struct fields\n",
        "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
        "schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructField(food,StringType,true), StructField(price,IntegerType,true)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCl8EUIqVJeL",
        "outputId": "0833f19b-a5e6-4e44-9143-17acd0fd0f3a"
      },
      "source": [
        "# Pass in our fields\n",
        "final = StructType(fields=schema)\n",
        "final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(food,StringType,true),StructField(price,IntegerType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coSErcceVSxv",
        "outputId": "68c08d8d-e562-4431-f555-7bf084fa0188"
      },
      "source": [
        "# Read in the data again\n",
        "# Read our data with our new schema\n",
        "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
        "dataframe.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- food: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ion3pUVskP",
        "outputId": "5af79c21-4288-4c3b-c114-44075fe9f274"
      },
      "source": [
        "# Different ways to access our data with Spark\n",
        "\n",
        "# dataframe['price']\n",
        "# type(dataframe['price'])\n",
        "# dataframe.select('price')\n",
        "#type(dataframe.select('price'))\n",
        "dataframe.select('price').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|price|\n",
            "+-----+\n",
            "|    0|\n",
            "|   12|\n",
            "|   10|\n",
            "+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qif9pOdyXbJi",
        "outputId": "65288ad6-2ae8-47c4-d4df-b9b3d9ef8a27"
      },
      "source": [
        "# To manipulate columns\n",
        "# Add new column\n",
        "dataframe.withColumn('newprice', dataframe['price']).show()\n",
        "# Update column name\n",
        "dataframe.withColumnRenamed('price','newerprice').show()\n",
        "# Double the price\n",
        "dataframe.withColumn('doubleprice',dataframe['price']*2).show()\n",
        "# Add a dollar to the price\n",
        "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()\n",
        "# Half the price\n",
        "dataframe.withColumn('half_price',dataframe['price']/2).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+--------+\n",
            "|   food|price|newprice|\n",
            "+-------+-----+--------+\n",
            "|  pizza|    0|       0|\n",
            "|  sushi|   12|      12|\n",
            "|chinese|   10|      10|\n",
            "+-------+-----+--------+\n",
            "\n",
            "+-------+----------+\n",
            "|   food|newerprice|\n",
            "+-------+----------+\n",
            "|  pizza|         0|\n",
            "|  sushi|        12|\n",
            "|chinese|        10|\n",
            "+-------+----------+\n",
            "\n",
            "+-------+-----+-----------+\n",
            "|   food|price|doubleprice|\n",
            "+-------+-----+-----------+\n",
            "|  pizza|    0|          0|\n",
            "|  sushi|   12|         24|\n",
            "|chinese|   10|         20|\n",
            "+-------+-----+-----------+\n",
            "\n",
            "+-------+-----+--------------+\n",
            "|   food|price|add_one_dollar|\n",
            "+-------+-----+--------------+\n",
            "|  pizza|    0|             1|\n",
            "|  sushi|   12|            13|\n",
            "|chinese|   10|            11|\n",
            "+-------+-----+--------------+\n",
            "\n",
            "+-------+-----+----------+\n",
            "|   food|price|half_price|\n",
            "+-------+-----+----------+\n",
            "|  pizza|    0|       0.0|\n",
            "|  sushi|   12|       6.0|\n",
            "|chinese|   10|       5.0|\n",
            "+-------+-----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMjyXLfqNN6r"
      },
      "source": [
        "16.4.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEDDmaAHlpd0"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRpXQ2itmIuG",
        "outputId": "cf3433ce-0101-4f42-f1c3-20a5b62640b9"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|     US|Mac Watson honors...|Special Selected ...|    96|   90|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|     US|This spent 20 mon...|             Reserve|    96|   65|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "| France|This is the top w...|         La Br��lade|    95|   66|          Provence|              Bandol|             null|Provence red blend|Domaine de la B̩gude|\n",
            "|  Spain|Deep, dense and p...|           Numanthia|    95|   73|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
            "|  Spain|Slightly gritty b...|          San Rom��n|    95|   65|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
            "|  Spain|Lush cedary black...|Carodorum �_nico ...|    95|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|     US|This re-named vin...|              Silice|    95|   65|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergstr̦m|\n",
            "|     US|The producer sour...|Gap's Crown Vineyard|    95|   60|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "|  Italy|Elegance, complex...|  Ronco della Chiesa|    95|   80|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
            "|     US|From 18-year-old ...|Estate Vineyard W...|    95|   48|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "|     US|A standout even i...|      Weber Vineyard|    95|   48|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| France|This wine is in p...|Ch̢teau Montus Pr...|    95|   90|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
            "|     US|With its sophisti...|      Grace Vineyard|    95|  185|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "|     US|First made in 200...|              Sigrid|    95|   90|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergstr̦m|\n",
            "|     US|This blockbuster,...|     Rainin Vineyard|    95|  325|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "|  Spain|Nicely oaked blac...|6 A̱os Reserva Pr...|    95|   80|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
            "| France|Coming from a sev...|       Le Pigeonnier|    95|  290|  Southwest France|              Cahors|             null|            Malbec|  Ch̢teau Lagr̩zette|\n",
            "|     US|This fresh and li...|Gap's Crown Vineyard|    95|   75|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ7t6PdANTM8"
      },
      "source": [
        " now we'll use this data to identify the difference btw transformation and actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItQVEGTi8X40",
        "outputId": "7c0df61b-0a54-41cd-9a11-9b19e9562537"
      },
      "source": [
        "# Order a DataFrame by ascending values\n",
        "df.orderBy(df[\"points\"].desc())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[country: string, description: string, designation: string, points: string, price: string, province: string, region_1: string, region_2: string, variety: string, winery: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mlaym3f9KXA",
        "outputId": "7b9330c4-819f-4add-d79a-91ca114c21d3"
      },
      "source": [
        "df.orderBy(df[\"points\"].desc()).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "|country|         description|         designation|points|price|  province|   region_1|region_2|             variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "|     US|This is an absolu...|           IX Estate|    99|  290|California|Napa Valley|    Napa|           Red Blend|              Colgin|\n",
            "| France|98-100 Barrel sam...|       Barrel sample|    99| null|  Bordeaux|   Pauillac|    null|Bordeaux-style Re...|Ch̢teau Pontet-Canet|\n",
            "|     US|There are incredi...|Elevation 1147 Es...|    99|  150|California|Napa Valley|    Napa|  Cabernet Sauvignon|        David Arthur|\n",
            "| France|A magnificent Cha...|Dom P̩rignon Oeno...|    99|  385| Champagne|  Champagne|    null|     Champagne Blend|     Mo��t & Chandon|\n",
            "|  Italy|Even better than ...|             Masseto|    99|  250|   Tuscany|    Toscana|    null|              Merlot|Tenuta dell'Ornel...|\n",
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILfEgjfk-tiA",
        "outputId": "3746a509-b09b-4038-e918-00010156cdaa"
      },
      "source": [
        "# More functions \n",
        "# Import functions\n",
        "from pyspark.sql.functions import avg\n",
        "df.select(avg(\"points\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|      avg(points)|\n",
            "+-----------------+\n",
            "|87.88834105383143|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DRj_EMALW4",
        "outputId": "e52ccf8a-0d99-4259-f3d8-a6ae36918c1e"
      },
      "source": [
        "# Filter (Spark can filter on columns \n",
        "# by supplying name of column and operator on what to compare it against)\n",
        "df.filter(\"price<20\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "| country|         description|designation|points|price|  province|            region_1|         region_2|       variety|              winery|\n",
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "|Bulgaria|This Bulgarian Ma...|    Bergul̩|    90|   15|  Bulgaria|                null|             null|        Mavrud|        Villa Melnik|\n",
            "|   Spain|Earthy plum and c...|     Amandi|    90|   17|   Galicia|       Ribeira Sacra|             null|       Menc�_a|      Don Bernardino|\n",
            "|      US|There's a lot to ...|       null|    90|   18|California|Russian River Valley|           Sonoma|    Chardonnay|            De Loach|\n",
            "|      US|Massively fruity,...|       null|    91|   19|    Oregon|   Willamette Valley|Willamette Valley|    Pinot Gris|   Trinity Vineyards|\n",
            "|Portugal|It is the ripe da...|    Premium|    91|   15|  Alentejo|                null|             null|Portuguese Red|Adega Cooperativa...|\n",
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMH5fJ2pAu7E",
        "outputId": "cf30fe37-34e5-42ab-9186-cada006e00c5"
      },
      "source": [
        "# Filter by price on certain columns\n",
        "df. filter(\"price<20\").select(['points', 'country', 'winery', 'price']).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+--------------------+-----+\n",
            "|points| country|              winery|price|\n",
            "+------+--------+--------------------+-----+\n",
            "|    90|Bulgaria|        Villa Melnik|   15|\n",
            "|    90|   Spain|      Don Bernardino|   17|\n",
            "|    90|      US|            De Loach|   18|\n",
            "|    91|      US|   Trinity Vineyards|   19|\n",
            "|    91|Portugal|Adega Cooperativa...|   15|\n",
            "+------+--------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n05Z1eOsMxFE"
      },
      "source": [
        " Above codes using filter() is SQL context with price<20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSni9rgMB4r9",
        "outputId": "0f9f32ef-356f-466b-b935-d61ce73bd8ee"
      },
      "source": [
        "# Now we perform the same transformations using Python:\n",
        "\n",
        "# Filter\n",
        "df.filter(\"price<20\").show(5)\n",
        "# Filter by price on certain columns\n",
        "df.filter(\"price<20\").select(['points','country', 'winery','price']).show(5)\n",
        "\n",
        "# Filter on exact state\n",
        "df.filter(df[\"country\"] == \"US\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "| country|         description|designation|points|price|  province|            region_1|         region_2|       variety|              winery|\n",
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "|Bulgaria|This Bulgarian Ma...|    Bergul̩|    90|   15|  Bulgaria|                null|             null|        Mavrud|        Villa Melnik|\n",
            "|   Spain|Earthy plum and c...|     Amandi|    90|   17|   Galicia|       Ribeira Sacra|             null|       Menc�_a|      Don Bernardino|\n",
            "|      US|There's a lot to ...|       null|    90|   18|California|Russian River Valley|           Sonoma|    Chardonnay|            De Loach|\n",
            "|      US|Massively fruity,...|       null|    91|   19|    Oregon|   Willamette Valley|Willamette Valley|    Pinot Gris|   Trinity Vineyards|\n",
            "|Portugal|It is the ripe da...|    Premium|    91|   15|  Alentejo|                null|             null|Portuguese Red|Adega Cooperativa...|\n",
            "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+--------+--------------------+-----+\n",
            "|points| country|              winery|price|\n",
            "+------+--------+--------------------+-----+\n",
            "|    90|Bulgaria|        Villa Melnik|   15|\n",
            "|    90|   Spain|      Don Bernardino|   17|\n",
            "|    90|      US|            De Loach|   18|\n",
            "|    91|      US|   Trinity Vineyards|   19|\n",
            "|    91|Portugal|Adega Cooperativa...|   15|\n",
            "+------+--------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
            "|country|         description|         designation|points|price|  province|            region_1|         region_2|           variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
            "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|     US|Mac Watson honors...|Special Selected ...|    96|   90|California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|     US|This spent 20 mon...|             Reserve|    96|   65|    Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "|     US|This re-named vin...|              Silice|    95|   65|    Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergstr̦m|\n",
            "|     US|The producer sour...|Gap's Crown Vineyard|    95|   60|California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "|     US|From 18-year-old ...|Estate Vineyard W...|    95|   48|    Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "|     US|A standout even i...|      Weber Vineyard|    95|   48|    Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "|     US|With its sophisti...|      Grace Vineyard|    95|  185|    Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "|     US|First made in 200...|              Sigrid|    95|   90|    Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergstr̦m|\n",
            "|     US|This blockbuster,...|     Rainin Vineyard|    95|  325|California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "|     US|This fresh and li...|Gap's Crown Vineyard|    95|   75|California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "|     US|Heitz has made th...|          Grignolino|    95|   24|California|         Napa Valley|             Napa|              Ros̩|               Heitz|\n",
            "|     US|The apogee of thi...|       Giallo Solare|    95|   60|California|         Edna Valley|    Central Coast|        Chardonnay|    Center of Effort|\n",
            "|     US|San Jose-based pr...|       R-Bar-R Ranch|    95|   45|California|Santa Cruz Mountains|    Central Coast|        Pinot Noir|            Comartin|\n",
            "|     US|Bergstr̦m has mad...|       Shea Vineyard|    94|   62|    Oregon|   Willamette Valley|             null|        Pinot Noir|           Bergstr̦m|\n",
            "|     US|Focused and dense...|             Abetina|    94|  105|    Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "|     US|Cranberry, baked ...|     Garys' Vineyard|    94|   60|California|Santa Lucia Highl...|    Central Coast|        Pinot Noir|                Roar|\n",
            "|     US|This standout Roc...|     The Funk Estate|    94|   60|Washington|Walla Walla Valle...|  Columbia Valley|             Syrah|              Saviah|\n",
            "|     US|Steely and perfum...|            Babushka|    90|   37|California|Russian River Valley|           Sonoma|        Chardonnay|            Zepaltas|\n",
            "|     US|The aromas entice...| Conner Lee Vineyard|    90|   42|Washington|Columbia Valley (WA)|  Columbia Valley|        Chardonnay|                Buty|\n",
            "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kh3LyfCFbwD"
      },
      "source": [
        "16.5.3 \n",
        "Natural Language Toolkit library (install) \n",
        "First three lines of code not from module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3M2JbFyPKmC",
        "outputId": "4df10e7f-4110-4d72-936d-c6353d54ad73"
      },
      "source": [
        "# 16.5.3 \n",
        "# Natural Language Toolkit library (install) \n",
        "# First three lines of code not from module\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "text = word_tokenize(\"I enjoy biking on the trails\")\n",
        "output = nltk.pos_tag(text)\n",
        "print(output)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('I', 'PRP'), ('enjoy', 'VBP'), ('biking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('trails', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdxWdQUhMpW6"
      },
      "source": [
        "16.6.1 Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeLjPbdKaRSq"
      },
      "source": [
        "# import os\n",
        "# # Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# # For example:\n",
        "# # spark_version = 'spark-3.0.2'\n",
        "# spark_version = 'spark-3.1.2'\n",
        "# os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# # Install Spark and Java\n",
        "# !apt-get update\n",
        "# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "# !tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "\n",
        "# # Set Environment Variables\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# # Start a SparkSession\n",
        "# import findspark\n",
        "# findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSX2cFRajcV"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFAL27P5apDh"
      },
      "source": [
        "# Now import the Tokenizer library\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcOxVimRHPEn",
        "outputId": "8a68709d-9862-4663-d0f0-c4fe00dbeda9"
      },
      "source": [
        "# Create sample dataFrame from scratch\n",
        "dataframe = spark.createDataFrame([\n",
        "    (0, \"Spark is great\"),\n",
        "    (1, \"We are learning Spark\"),\n",
        "    (2, \"Spark is better than Hadoop no doubt\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "dataframe.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|      Spark is great|\n",
            "|  1|We are learning S...|\n",
            "|  2|Spark is better t...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b98wPZaJRsJ",
        "outputId": "6d82574d-667e-48a2-f019-f01bd51802f1"
      },
      "source": [
        "# Tokenize sentences\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_c76ab11e6e84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U7eE4FSMcmr"
      },
      "source": [
        " The tokenizer that we created uses a transform method \n",
        " that takes a DataFrame as input. This is a transformation, \n",
        " so to reveal the results, we'll call show(truncate=False) \n",
        " as our action to display the results without shortening \n",
        " the output, as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF29h0ndKBb9",
        "outputId": "91d6e451-6e2e-425d-a043-ba0071414f5d"
      },
      "source": [
        "# transform and show DataFrame (note, dataframe is the \n",
        "# 0ne we created from scratch 2 cells above)\n",
        "tokenized_df = tokenizer.transform(dataframe)\n",
        "tokenized_df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------------------------+--------------------------------------------+\n",
            "|id |sentence                            |words                                       |\n",
            "+---+------------------------------------+--------------------------------------------+\n",
            "|0  |Spark is great                      |[spark, is, great]                          |\n",
            "|1  |We are learning Spark               |[we, are, learning, spark]                  |\n",
            "|2  |Spark is better than Hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|\n",
            "+---+------------------------------------+--------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXzc7-NnLS7t"
      },
      "source": [
        "# Create a function to return the length of a list\n",
        "def word_list_length(word_list):\n",
        "    return len(word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBXqxy2LlK1"
      },
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNuuYftMSJk"
      },
      "source": [
        "Using the udf function, we can create our function to be passed in. The udf will take in the name of the function as a parameter and the output data type, which is the IntegerType that we just imported. Type and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMCPVXh0Lx8i"
      },
      "source": [
        "# Create a user defined function\n",
        "count_tokens = udf(word_list_length, IntegerType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyHkJFEJNgNj"
      },
      "source": [
        " Now we can redo the tokenizer process. Only this time, \n",
        "after the DataFrame has outputted the tokenized values, \n",
        "we can use our own created function to return the number \n",
        "of tokens created. This will give us another data point to \n",
        "use in the future, if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_5nc26NrZT",
        "outputId": "93500ebf-855e-44a4-8e76-b84259c2383b"
      },
      "source": [
        "# create our Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "# Transform DataFrame\n",
        "tokenized_df = tokenizer.transform(dataframe)\n",
        "\n",
        "# Select the needed columns and don't truncate results\n",
        "tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------------------------+--------------------------------------------+------+\n",
            "|id |sentence                            |words                                       |tokens|\n",
            "+---+------------------------------------+--------------------------------------------+------+\n",
            "|0  |Spark is great                      |[spark, is, great]                          |3     |\n",
            "|1  |We are learning Spark               |[we, are, learning, spark]                  |4     |\n",
            "|2  |Spark is better than Hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|7     |\n",
            "+---+------------------------------------+--------------------------------------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_iiWvtQO3c"
      },
      "source": [
        "!6.6.2 Stop Words:\n",
        "\n",
        "Stop word removal code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Txu9_QVQd4z"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGAFD4w1Q-C6"
      },
      "source": [
        "Next, create a DataFrame that's already a list of words. By creating a list of words, we can skip the tokenization step for now. As you recall, tokenization takes input and separates it into a list of words. By creating a DataFrame that already contains a list of words, we are replicating this step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7l6iNDlRBgh",
        "outputId": "1a559b7a-79c2-459e-d933-a98357b08e8b"
      },
      "source": [
        "# Create DataFrame\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"Big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
        "    (1, [\"This\", \"is\", \"going\", \"to\", \"be\", \"epic\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "sentenceData.show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------------------+\n",
            "|id |raw                             |\n",
            "+---+--------------------------------+\n",
            "|0  |[Big, data, is, super, powerful]|\n",
            "|1  |[This, is, going, to, be, epic] |\n",
            "+---+--------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff-W0S4hSgeN"
      },
      "source": [
        "Now import the StopWordsRemover library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH9bXqobSj4O"
      },
      "source": [
        "# Import stop words library\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf-rAp-BSqjT"
      },
      "source": [
        "Then, run the StopWordsRemover() function, which takes an input column that will be passed into the function, and an output column to add the results. This is stored in a variable for ease of use later:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzshOjLISuQ0"
      },
      "source": [
        "# Run the Remover\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OAaV0zDS2VN"
      },
      "source": [
        "Now transform the DataFrame by applying StopWordsRemover and display the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM3uTw_US4uI",
        "outputId": "ccd7a9b4-62a6-44c1-d400-352ab6a013fc"
      },
      "source": [
        "# Transform and show data\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------------------+----------------------------+\n",
            "|id |raw                             |filtered                    |\n",
            "+---+--------------------------------+----------------------------+\n",
            "|0  |[Big, data, is, super, powerful]|[Big, data, super, powerful]|\n",
            "|1  |[This, is, going, to, be, epic] |[going, epic]               |\n",
            "+---+--------------------------------+----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyx3qXcSTRD0"
      },
      "source": [
        "The outcome shows the raw data, or input, and the result of running the StopWordsRemover() function. In the filtered column, all the stop words are removed and the result is displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJaiG-Dd--S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fgJpw7TdwJS"
      },
      "source": [
        "16.6.3 \n",
        "TF-IDF Weight\n",
        "\n",
        "HashingTF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3na0R3geNjo"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"TF-IDF\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_8BtEPGeaWx"
      },
      "source": [
        "Import the following libraries that will be used, and go through both the tokenizer and stop words steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M62fIeeGeVxK"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FviaRtgLeqt6",
        "outputId": "1349335f-a972-4c06-9aeb-f6503d71433b"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/airlines.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"airlines.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|      Airline Tweets|\n",
            "+--------------------+\n",
            "|@VirginAmerica pl...|\n",
            "|@VirginAmerica se...|\n",
            "|@VirginAmerica do...|\n",
            "|@VirginAmerica Ar...|\n",
            "|@VirginAmerica aw...|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbYStIiZewYb"
      },
      "source": [
        "We'll tokenize our data first, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk8juuslezR9",
        "outputId": "61dcdde7-1d9e-4395-9070-4c6382256031"
      },
      "source": [
        "# Tokenize DataFrame\n",
        "tokened = Tokenizer(inputCol=\"Airline Tweets\", outputCol=\"words\")\n",
        "tokened_transformed = tokened.transform(df)\n",
        "tokened_transformed.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|      Airline Tweets|               words|\n",
            "+--------------------+--------------------+\n",
            "|@VirginAmerica pl...|[@virginamerica, ...|\n",
            "|@VirginAmerica se...|[@virginamerica, ...|\n",
            "|@VirginAmerica do...|[@virginamerica, ...|\n",
            "|@VirginAmerica Ar...|[@virginamerica, ...|\n",
            "|@VirginAmerica aw...|[@virginamerica, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mK_NU83e9kp",
        "outputId": "d1a956e1-b19e-4951-b67e-313c1957aa46"
      },
      "source": [
        "# Next,  Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "removed_frame = remover.transform(tokened_transformed)\n",
        "removed_frame.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |\n",
            "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |\n",
            "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |\n",
            "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |\n",
            "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50YxKn9Cfld_",
        "outputId": "1aee4552-ddf5-4eb3-91e1-0da936f596f2"
      },
      "source": [
        "# Run the hashing term frequency\n",
        "hashing = HashingTF(inputCol=\"filtered\",outputCol=\"hashedValues\",numFeatures=pow(2,18))\n",
        "\n",
        "# Transform into DF\n",
        "hashed_df = hashing.transform(removed_frame)\n",
        "hashed_df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |hashedValues                                                                                                                               |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |(262144,[1419,99916,168274,171322,186669,256498],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                |\n",
            "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |(262144,[107065,117975,147224,171322,200547,232735],[1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n",
            "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                            |\n",
            "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id4lFVirhatr"
      },
      "source": [
        "Above >> If you scroll to the right and down, you'll see the hashedValues result, which contains the index and term frequency of the corresponding index for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YSAl2VhjJA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFztTj5uhkGc"
      },
      "source": [
        "With our words successfully converted to numbers, we can plug it all into an IDFModel, which will scale the values while down-weighting based on document frequency. If you scroll to the right on the output, you'll see the result of the TF-IDF:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37Oucn9hn5c",
        "outputId": "8588bb5b-8956-4eee-b908-955b3c2c18f5"
      },
      "source": [
        "# Fit the IDF on the data set\n",
        "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel = idf.fit(hashed_df)\n",
        "rescaledData = idfModel.transform(hashed_df)\n",
        "\n",
        "# Display the dataframe\n",
        "rescaledData.select(\"words\", \"features\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|words                                                                                                                                                          |features                                                                                                                                                                                                                                                                                                        |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |(262144,[1419,99916,168274,171322,186669,256498],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                          |\n",
            "|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098])|\n",
            "|[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |(262144,[107065,117975,147224,171322,200547,232735],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                       |\n",
            "|[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                                                                                                        |\n",
            "|[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0986122886681098,1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                           |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k_EZStji-OH"
      },
      "source": [
        "**Remember that computers can't just read text and analyze it. With the process we have just completed, we have now given values from raw text that a computer can work with.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTHzGKXId67a"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sMdDwNdi_qX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSd1RQhjWdM"
      },
      "source": [
        "**16.6.4**\n",
        "Set Up the Pipeline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIQwe8kdjfN2"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wx1swjskpuD",
        "outputId": "b32394ba-6c67-4225-d232-7d9392dfd664"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuhpddRVk7x8"
      },
      "source": [
        "Import all the functions that will be used in our NLP process, or pipeline, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0VXNfOtk9eH"
      },
      "source": [
        "# Import functions\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB4hIO-zlF5W"
      },
      "source": [
        "Next, create a new column that uses the lengthfunction to create a future feature with the length of each row. This is similar to the tokenizerphase when we created our own udfto do the same thing. A udfcould still be used here, but PySpark makes it easier by supplying a ready-to-use function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWZQlxNAlKJI",
        "outputId": "ac41d873-8008-4816-c88d-2aa9853cfdb7"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature\n",
        "data_df = df.withColumn('length', length(df['text']))\n",
        "data_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH4RmwLWlc6j"
      },
      "source": [
        "Now we'll create all the transformations to be applied in our pipeline. Note that the StringIndexer encodes a string column to a column of table indexes. Here we are working with positive and negative game reviews, which will be converted to 0 and 1. This will form our labels, The label is what we're trying to predict: will the review's given text let us know if it was positive or negative?\n",
        "\n",
        "Also note that we don't need to run all of these completely as we did before. By creating all the functions now, we can then use them all in the pipeline later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usjxOeXYljp_"
      },
      "source": [
        "# Create all the features to the data set\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m8zN3v0lv7A"
      },
      "source": [
        "We'll create a feature vector containing the output from the IDFModel (the last stage in the pipeline) and the length. This will combine all the raw features to train the ML model that we'll be using. Enter the code for this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVSIbzzulyeT"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OE7SZ0xl7rX"
      },
      "source": [
        "Now it's time to create our pipeline, the easiest step. We'll import the pipeline from pyspark.ml, and then store a list of the stages created earlier. It's important to list the stages in the order they need to be executed. As we mentioned before, the output from one stage will then be passed off to another stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgUmEWzil9Ik"
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2cAFYOe8d5K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IyszwiXGu05"
      },
      "source": [
        "16.6.5 Run the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC4Lz-1IHByA"
      },
      "source": [
        "After our pipeline has been set up, we'll fit the outcome with our original DataFrame and transform it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWeDnrkJZXK"
      },
      "source": [
        "As you can see in the following image, our labels and features that we created early on in the process are numerical representations of positive and negative reviews. The features will be used in our model and predict whether a given review will be positive or negative. These features are the result of all the work we have been doing with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIdakznKG1Po"
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(data_df)\n",
        "cleaned = cleaner.transform(data_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCyBn2YzLEX-"
      },
      "source": [
        "Now, To split the data into a training set and a testing set, run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmxKNj4ILKDm"
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98QudYkMLQDd"
      },
      "source": [
        "The array supplied to randomSplit is the percentage of the data that will be broken into training and testing respectively. So 70% to training and 30% to testing. The second number supplied is called a seed. The seed number here, 21, is arbitrary. But as long as the same seed is used, the result will be the same each time. Using a seed number ensures reproducible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPgMLmiLhZF"
      },
      "source": [
        "The ML model we'll use is Naive Bayes, which we'll import and then fit the model using the training dataset. Naive Bayes is a group of classifier algorithms based on Bayes' theorem. Bayes theorem provides a way to determine the probability of an event based on new conditions or information that might be related to the event.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3ThwgGiLi7x"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ__-hHgLokj"
      },
      "source": [
        "Once the model has been trained, we'll transform the model with our testing data. Run the following code, and then look at the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGKjbtNxLsyb",
        "outputId": "58b67d3a-da5c-4a93-bc16-af743bbe67c8"
      },
      "source": [
        "# Transform the Model with the testing  data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative|\"The burger... I ...|    86|  0.0|[\"the, burger...,...|[\"the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-820.60780566975...|[0.99999999999995...|       0.0|\n",
            "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-73.489435340867...|[0.07515735596910...|       1.0|\n",
            "|negative|After I pulled up...|    83|  0.0|[after, i, pulled...|[pulled, car, wai...|(262144,[65645,71...|(262144,[65645,71...|(262145,[65645,71...|[-620.40646705112...|[1.0,1.9205984091...|       0.0|\n",
            "|negative|Also, I feel like...|    58|  0.0|[also,, i, feel, ...|[also,, feel, lik...|(262144,[61899,66...|(262144,[61899,66...|(262145,[61899,66...|[-528.59562125515...|[0.99999999994873...|       0.0|\n",
            "|negative|Anyway, I do not ...|    44|  0.0|[anyway,, i, do, ...|[anyway,, think, ...|(262144,[132270,1...|(262144,[132270,1...|(262145,[132270,1...|[-334.09599709326...|[0.99999999994185...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQnCxrqyMFOv"
      },
      "source": [
        "At first glance, it may seem like the model isn't showing us anything useful. From your output, scroll all the way to the right to view the prediction column:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvU-8P6zMOJk"
      },
      "source": [
        "This prediction column will indicate with a 1.0 if the model thinks this review is negative and 0.0 if it thinks it's positive. Future data sets can now be run with this model and determine whether a review was positive or negative without having already supplied in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlZTpI5hMV4S"
      },
      "source": [
        "The last step is to import the BinaryClassificationEvaluator, which will display how accurate our model is in determining if a review with be positive or negative based solely on the text within a review.\n",
        "\n",
        "The BinaryClassificationEvaluator uses two arguments, labelCol and rawPredictionCol. The labelCol takes the labels which were the result of using StringIndexer to convert our positive and negative strings to integers. The rawPredictionCol takes in numerical predictions from the output of running the Naive Bayes model.\n",
        "\n",
        "The performance of a model can be measured based on the difference between its predicted values and actual values. This is what the BinaryClassificationEvaluator does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnjtD_yfMn2S",
        "outputId": "e3abec72-833c-427b-e505-76755426e360"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "acc_eval = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='prediction')\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of Model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Model at predicting reviews was: 0.700298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GilYYwFdOrsr"
      },
      "source": [
        "The accuracy of the model isn't perfect, but it's not too low either: 0.700298. Machine learning isn't a guarantee, and tweaking our models as well as the data used is part of the process. One of the ways to do this is to add more data; when you keep adding data, eventually you grow from your local storage to something much larger—thus leading to big data!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heTB5du0QyVv"
      },
      "source": [
        "16.7.1\n",
        "Evaluate Amazon Web Services"
      ]
    }
  ]
}